{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10538b55",
   "metadata": {},
   "source": [
    "# Analyzing the e-commerce Big Data Using Spark (Running on Local Machine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de923e13",
   "metadata": {},
   "source": [
    "### 1. Goals\n",
    "* In this notebook, we're going to go through the Olist Ecommerce Data with more than 4 millions records (almost 2Gb of data) with the goal of understanding the data and doing the data transformations by using PySpark. \n",
    "\n",
    "* Spark and Hadoop have been installed and set up locally, due to the limit of local machine, I struggled to handle the data at the beginning but utimately already overcomed the issues to deliver the rough analyst of the e-commerce data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0c71bb",
   "metadata": {},
   "source": [
    "### 2 . Data\n",
    "Data being used is merged from different datasets namely customers dataset, order_item dataset, product dataset, seller_dataset...into one to create the most holistic view of a transaction.\n",
    "\n",
    "The more details could be find at: https://www.kaggle.com/datasets/olistbr/brazilian-ecommerce/code\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef0ce21",
   "metadata": {},
   "source": [
    "### 3.Data Analysis\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90157225",
   "metadata": {},
   "source": [
    "### Configuration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "9c3e60a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#install the necessary libraries\n",
    "#!pip3 install pyspark==3.2.1\n",
    "#!pip install findspark\n",
    "#!pip3 install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f1170b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the libaries\n",
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "#import pyspark.sql.functions as F\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql.functions import *\n",
    "#from pyspark.sql.functions import countDistinct \n",
    "from pyspark.sql import SparkSession \n",
    "\n",
    "\n",
    "#SparkContext.setSystemProperty('spark.executor.memory', '4g')\n",
    "#SparkContext.setSystemProperty('spark.driver.maxResultSize', '10g')\n",
    "#SparkContext.stop(sc)\n",
    "#SparkContext.sc.getConf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ba3a620",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up Spark Session and Context\n",
    "spark = SparkSession.builder.master(\"local[24]\").appName(\"Test Spark\").getOrCreate()\n",
    "sc=spark.sparkContext\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7400df3",
   "metadata": {},
   "source": [
    "**Note**: Sets the Spark master to connect “local” to run locally, “local[24]” to run locally with 24 cores. At default, spark might not run on all the cpu cores, that will impact to the data processing speed, so I need to modify the running core to the optimal number. Setting local[*] is also another way to let spark run on all cpu cores.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80b5668b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-RG2MEJH.mshome.net:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[24]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Test Spark</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x18945207fd0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82165ae",
   "metadata": {},
   "source": [
    "### Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "469ed39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the data locally\n",
    "data = spark.read.csv(\"F:/Universities/Monash Study Materials/Monash - Chien/S1 - 2020/Data Visualization/Visualization Project/olist_customers_dataset.csv/full_data_1.csv\",inferSchema=True,header=True)\n",
    "#data.write.parquet(\"olistEcommerce.parquet\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f37a44c",
   "metadata": {},
   "source": [
    "**Note**: The reading data speed could be increased if we defined the Schema in advance, However in this case, I set the inferSchema=True to let Spark guess the data schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd209ca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Number of Records: integer (nullable = true)\n",
      " |-- amount: integer (nullable = true)\n",
      " |-- customer_city: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- customer_state: string (nullable = true)\n",
      " |-- customer_unique_id: string (nullable = true)\n",
      " |-- customer_zip_code_prefix: integer (nullable = true)\n",
      " |-- freight_value: double (nullable = true)\n",
      " |-- index: integer (nullable = true)\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- order_purchase_timestamp: string (nullable = true)\n",
      " |-- order_year_and_month: string (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- product_category_name_english: string (nullable = true)\n",
      " |-- product_id: string (nullable = true)\n",
      " |-- seller_Cluster: string (nullable = true)\n",
      " |-- seller_city: string (nullable = true)\n",
      " |-- seller_id: string (nullable = true)\n",
      " |-- seller_revenue: double (nullable = true)\n",
      " |-- seller_state: string (nullable = true)\n",
      " |-- seller_zip_code_prefix: integer (nullable = true)\n",
      " |-- shipping_limit_date: string (nullable = true)\n",
      " |-- sum_seller_revenue: double (nullable = true)\n",
      " |-- geolocation_lat: double (nullable = true)\n",
      " |-- geolocation_lng: double (nullable = true)\n",
      " |-- geolocation_city: string (nullable = true)\n",
      " |-- geolocation_state: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Check the data Schema \n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ede325",
   "metadata": {},
   "source": [
    "**Note**: Could be seen that when letting the machine guess the schema, some of the datatypes have been mixed up such as order_purchase_timestamp, order_year_and_month, etc.. we will need to convert them to the dateandtime datatype later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae872dcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4862195"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#count the number of row\n",
    "data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fe9749b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Number of Records</th>\n",
       "      <th>amount</th>\n",
       "      <th>customer_city</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>customer_state</th>\n",
       "      <th>customer_unique_id</th>\n",
       "      <th>customer_zip_code_prefix</th>\n",
       "      <th>freight_value</th>\n",
       "      <th>index</th>\n",
       "      <th>order_id</th>\n",
       "      <th>...</th>\n",
       "      <th>seller_id</th>\n",
       "      <th>seller_revenue</th>\n",
       "      <th>seller_state</th>\n",
       "      <th>seller_zip_code_prefix</th>\n",
       "      <th>shipping_limit_date</th>\n",
       "      <th>sum_seller_revenue</th>\n",
       "      <th>geolocation_lat</th>\n",
       "      <th>geolocation_lng</th>\n",
       "      <th>geolocation_city</th>\n",
       "      <th>geolocation_state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Natal</td>\n",
       "      <td>92de9528e4b8f2568745e095beb94aeb</td>\n",
       "      <td>RN</td>\n",
       "      <td>2d48cce6ed77d4963cbf11a7bbc979df</td>\n",
       "      <td>59014</td>\n",
       "      <td>29.93</td>\n",
       "      <td>71807</td>\n",
       "      <td>a96843a8dacaebe8ab691a745032daf5</td>\n",
       "      <td>...</td>\n",
       "      <td>7e93a43ef30c4f03f38b393420bc753a</td>\n",
       "      <td>1086.76</td>\n",
       "      <td>SP</td>\n",
       "      <td>6429</td>\n",
       "      <td>19/05/2017 4:05:18 PM</td>\n",
       "      <td>99028.52</td>\n",
       "      <td>-23.491792</td>\n",
       "      <td>-46.947333</td>\n",
       "      <td>barueri</td>\n",
       "      <td>SP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Natal</td>\n",
       "      <td>92de9528e4b8f2568745e095beb94aeb</td>\n",
       "      <td>RN</td>\n",
       "      <td>2d48cce6ed77d4963cbf11a7bbc979df</td>\n",
       "      <td>59014</td>\n",
       "      <td>29.93</td>\n",
       "      <td>71807</td>\n",
       "      <td>a96843a8dacaebe8ab691a745032daf5</td>\n",
       "      <td>...</td>\n",
       "      <td>7e93a43ef30c4f03f38b393420bc753a</td>\n",
       "      <td>1086.76</td>\n",
       "      <td>SP</td>\n",
       "      <td>6429</td>\n",
       "      <td>19/05/2017 4:05:18 PM</td>\n",
       "      <td>99028.52</td>\n",
       "      <td>-23.494575</td>\n",
       "      <td>-46.958754</td>\n",
       "      <td>barueri</td>\n",
       "      <td>SP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Natal</td>\n",
       "      <td>92de9528e4b8f2568745e095beb94aeb</td>\n",
       "      <td>RN</td>\n",
       "      <td>2d48cce6ed77d4963cbf11a7bbc979df</td>\n",
       "      <td>59014</td>\n",
       "      <td>29.93</td>\n",
       "      <td>71807</td>\n",
       "      <td>a96843a8dacaebe8ab691a745032daf5</td>\n",
       "      <td>...</td>\n",
       "      <td>7e93a43ef30c4f03f38b393420bc753a</td>\n",
       "      <td>1086.76</td>\n",
       "      <td>SP</td>\n",
       "      <td>6429</td>\n",
       "      <td>19/05/2017 4:05:18 PM</td>\n",
       "      <td>99028.52</td>\n",
       "      <td>-23.487011</td>\n",
       "      <td>-46.944571</td>\n",
       "      <td>barueri</td>\n",
       "      <td>SP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Natal</td>\n",
       "      <td>92de9528e4b8f2568745e095beb94aeb</td>\n",
       "      <td>RN</td>\n",
       "      <td>2d48cce6ed77d4963cbf11a7bbc979df</td>\n",
       "      <td>59014</td>\n",
       "      <td>29.93</td>\n",
       "      <td>71807</td>\n",
       "      <td>a96843a8dacaebe8ab691a745032daf5</td>\n",
       "      <td>...</td>\n",
       "      <td>7e93a43ef30c4f03f38b393420bc753a</td>\n",
       "      <td>1086.76</td>\n",
       "      <td>SP</td>\n",
       "      <td>6429</td>\n",
       "      <td>19/05/2017 4:05:18 PM</td>\n",
       "      <td>99028.52</td>\n",
       "      <td>-23.487624</td>\n",
       "      <td>-46.949395</td>\n",
       "      <td>barueri</td>\n",
       "      <td>SP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Natal</td>\n",
       "      <td>92de9528e4b8f2568745e095beb94aeb</td>\n",
       "      <td>RN</td>\n",
       "      <td>2d48cce6ed77d4963cbf11a7bbc979df</td>\n",
       "      <td>59014</td>\n",
       "      <td>29.93</td>\n",
       "      <td>71807</td>\n",
       "      <td>a96843a8dacaebe8ab691a745032daf5</td>\n",
       "      <td>...</td>\n",
       "      <td>7e93a43ef30c4f03f38b393420bc753a</td>\n",
       "      <td>1086.76</td>\n",
       "      <td>SP</td>\n",
       "      <td>6429</td>\n",
       "      <td>19/05/2017 4:05:18 PM</td>\n",
       "      <td>99028.52</td>\n",
       "      <td>-23.484640</td>\n",
       "      <td>-46.952437</td>\n",
       "      <td>barueri</td>\n",
       "      <td>SP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Natal</td>\n",
       "      <td>92de9528e4b8f2568745e095beb94aeb</td>\n",
       "      <td>RN</td>\n",
       "      <td>2d48cce6ed77d4963cbf11a7bbc979df</td>\n",
       "      <td>59014</td>\n",
       "      <td>29.93</td>\n",
       "      <td>71807</td>\n",
       "      <td>a96843a8dacaebe8ab691a745032daf5</td>\n",
       "      <td>...</td>\n",
       "      <td>7e93a43ef30c4f03f38b393420bc753a</td>\n",
       "      <td>1086.76</td>\n",
       "      <td>SP</td>\n",
       "      <td>6429</td>\n",
       "      <td>19/05/2017 4:05:18 PM</td>\n",
       "      <td>99028.52</td>\n",
       "      <td>-23.489361</td>\n",
       "      <td>-46.955389</td>\n",
       "      <td>barueri</td>\n",
       "      <td>SP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Natal</td>\n",
       "      <td>92de9528e4b8f2568745e095beb94aeb</td>\n",
       "      <td>RN</td>\n",
       "      <td>2d48cce6ed77d4963cbf11a7bbc979df</td>\n",
       "      <td>59014</td>\n",
       "      <td>29.93</td>\n",
       "      <td>71807</td>\n",
       "      <td>a96843a8dacaebe8ab691a745032daf5</td>\n",
       "      <td>...</td>\n",
       "      <td>7e93a43ef30c4f03f38b393420bc753a</td>\n",
       "      <td>1086.76</td>\n",
       "      <td>SP</td>\n",
       "      <td>6429</td>\n",
       "      <td>19/05/2017 4:05:18 PM</td>\n",
       "      <td>99028.52</td>\n",
       "      <td>-23.490570</td>\n",
       "      <td>-46.956202</td>\n",
       "      <td>barueri</td>\n",
       "      <td>SP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Natal</td>\n",
       "      <td>92de9528e4b8f2568745e095beb94aeb</td>\n",
       "      <td>RN</td>\n",
       "      <td>2d48cce6ed77d4963cbf11a7bbc979df</td>\n",
       "      <td>59014</td>\n",
       "      <td>29.93</td>\n",
       "      <td>71807</td>\n",
       "      <td>a96843a8dacaebe8ab691a745032daf5</td>\n",
       "      <td>...</td>\n",
       "      <td>7e93a43ef30c4f03f38b393420bc753a</td>\n",
       "      <td>1086.76</td>\n",
       "      <td>SP</td>\n",
       "      <td>6429</td>\n",
       "      <td>19/05/2017 4:05:18 PM</td>\n",
       "      <td>99028.52</td>\n",
       "      <td>-23.487478</td>\n",
       "      <td>-46.949237</td>\n",
       "      <td>barueri</td>\n",
       "      <td>SP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Natal</td>\n",
       "      <td>92de9528e4b8f2568745e095beb94aeb</td>\n",
       "      <td>RN</td>\n",
       "      <td>2d48cce6ed77d4963cbf11a7bbc979df</td>\n",
       "      <td>59014</td>\n",
       "      <td>29.93</td>\n",
       "      <td>71807</td>\n",
       "      <td>a96843a8dacaebe8ab691a745032daf5</td>\n",
       "      <td>...</td>\n",
       "      <td>7e93a43ef30c4f03f38b393420bc753a</td>\n",
       "      <td>1086.76</td>\n",
       "      <td>SP</td>\n",
       "      <td>6429</td>\n",
       "      <td>19/05/2017 4:05:18 PM</td>\n",
       "      <td>99028.52</td>\n",
       "      <td>-23.489821</td>\n",
       "      <td>-46.949699</td>\n",
       "      <td>barueri</td>\n",
       "      <td>SP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Natal</td>\n",
       "      <td>92de9528e4b8f2568745e095beb94aeb</td>\n",
       "      <td>RN</td>\n",
       "      <td>2d48cce6ed77d4963cbf11a7bbc979df</td>\n",
       "      <td>59014</td>\n",
       "      <td>29.93</td>\n",
       "      <td>71807</td>\n",
       "      <td>a96843a8dacaebe8ab691a745032daf5</td>\n",
       "      <td>...</td>\n",
       "      <td>7e93a43ef30c4f03f38b393420bc753a</td>\n",
       "      <td>1086.76</td>\n",
       "      <td>SP</td>\n",
       "      <td>6429</td>\n",
       "      <td>19/05/2017 4:05:18 PM</td>\n",
       "      <td>99028.52</td>\n",
       "      <td>-23.487354</td>\n",
       "      <td>-46.952158</td>\n",
       "      <td>barueri</td>\n",
       "      <td>SP</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Number of Records  amount customer_city                       customer_id  \\\n",
       "0                  1       1         Natal  92de9528e4b8f2568745e095beb94aeb   \n",
       "1                  1       1         Natal  92de9528e4b8f2568745e095beb94aeb   \n",
       "2                  1       1         Natal  92de9528e4b8f2568745e095beb94aeb   \n",
       "3                  1       1         Natal  92de9528e4b8f2568745e095beb94aeb   \n",
       "4                  1       1         Natal  92de9528e4b8f2568745e095beb94aeb   \n",
       "5                  1       1         Natal  92de9528e4b8f2568745e095beb94aeb   \n",
       "6                  1       1         Natal  92de9528e4b8f2568745e095beb94aeb   \n",
       "7                  1       1         Natal  92de9528e4b8f2568745e095beb94aeb   \n",
       "8                  1       1         Natal  92de9528e4b8f2568745e095beb94aeb   \n",
       "9                  1       1         Natal  92de9528e4b8f2568745e095beb94aeb   \n",
       "\n",
       "  customer_state                customer_unique_id  customer_zip_code_prefix  \\\n",
       "0             RN  2d48cce6ed77d4963cbf11a7bbc979df                     59014   \n",
       "1             RN  2d48cce6ed77d4963cbf11a7bbc979df                     59014   \n",
       "2             RN  2d48cce6ed77d4963cbf11a7bbc979df                     59014   \n",
       "3             RN  2d48cce6ed77d4963cbf11a7bbc979df                     59014   \n",
       "4             RN  2d48cce6ed77d4963cbf11a7bbc979df                     59014   \n",
       "5             RN  2d48cce6ed77d4963cbf11a7bbc979df                     59014   \n",
       "6             RN  2d48cce6ed77d4963cbf11a7bbc979df                     59014   \n",
       "7             RN  2d48cce6ed77d4963cbf11a7bbc979df                     59014   \n",
       "8             RN  2d48cce6ed77d4963cbf11a7bbc979df                     59014   \n",
       "9             RN  2d48cce6ed77d4963cbf11a7bbc979df                     59014   \n",
       "\n",
       "   freight_value  index                          order_id  ...  \\\n",
       "0          29.93  71807  a96843a8dacaebe8ab691a745032daf5  ...   \n",
       "1          29.93  71807  a96843a8dacaebe8ab691a745032daf5  ...   \n",
       "2          29.93  71807  a96843a8dacaebe8ab691a745032daf5  ...   \n",
       "3          29.93  71807  a96843a8dacaebe8ab691a745032daf5  ...   \n",
       "4          29.93  71807  a96843a8dacaebe8ab691a745032daf5  ...   \n",
       "5          29.93  71807  a96843a8dacaebe8ab691a745032daf5  ...   \n",
       "6          29.93  71807  a96843a8dacaebe8ab691a745032daf5  ...   \n",
       "7          29.93  71807  a96843a8dacaebe8ab691a745032daf5  ...   \n",
       "8          29.93  71807  a96843a8dacaebe8ab691a745032daf5  ...   \n",
       "9          29.93  71807  a96843a8dacaebe8ab691a745032daf5  ...   \n",
       "\n",
       "                          seller_id seller_revenue  seller_state  \\\n",
       "0  7e93a43ef30c4f03f38b393420bc753a        1086.76            SP   \n",
       "1  7e93a43ef30c4f03f38b393420bc753a        1086.76            SP   \n",
       "2  7e93a43ef30c4f03f38b393420bc753a        1086.76            SP   \n",
       "3  7e93a43ef30c4f03f38b393420bc753a        1086.76            SP   \n",
       "4  7e93a43ef30c4f03f38b393420bc753a        1086.76            SP   \n",
       "5  7e93a43ef30c4f03f38b393420bc753a        1086.76            SP   \n",
       "6  7e93a43ef30c4f03f38b393420bc753a        1086.76            SP   \n",
       "7  7e93a43ef30c4f03f38b393420bc753a        1086.76            SP   \n",
       "8  7e93a43ef30c4f03f38b393420bc753a        1086.76            SP   \n",
       "9  7e93a43ef30c4f03f38b393420bc753a        1086.76            SP   \n",
       "\n",
       "  seller_zip_code_prefix    shipping_limit_date sum_seller_revenue  \\\n",
       "0                   6429  19/05/2017 4:05:18 PM           99028.52   \n",
       "1                   6429  19/05/2017 4:05:18 PM           99028.52   \n",
       "2                   6429  19/05/2017 4:05:18 PM           99028.52   \n",
       "3                   6429  19/05/2017 4:05:18 PM           99028.52   \n",
       "4                   6429  19/05/2017 4:05:18 PM           99028.52   \n",
       "5                   6429  19/05/2017 4:05:18 PM           99028.52   \n",
       "6                   6429  19/05/2017 4:05:18 PM           99028.52   \n",
       "7                   6429  19/05/2017 4:05:18 PM           99028.52   \n",
       "8                   6429  19/05/2017 4:05:18 PM           99028.52   \n",
       "9                   6429  19/05/2017 4:05:18 PM           99028.52   \n",
       "\n",
       "  geolocation_lat geolocation_lng  geolocation_city geolocation_state  \n",
       "0      -23.491792      -46.947333           barueri                SP  \n",
       "1      -23.494575      -46.958754           barueri                SP  \n",
       "2      -23.487011      -46.944571           barueri                SP  \n",
       "3      -23.487624      -46.949395           barueri                SP  \n",
       "4      -23.484640      -46.952437           barueri                SP  \n",
       "5      -23.489361      -46.955389           barueri                SP  \n",
       "6      -23.490570      -46.956202           barueri                SP  \n",
       "7      -23.487478      -46.949237           barueri                SP  \n",
       "8      -23.489821      -46.949699           barueri                SP  \n",
       "9      -23.487354      -46.952158           barueri                SP  \n",
       "\n",
       "[10 rows x 27 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Showing the first 10 \n",
    "data.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66af3395",
   "metadata": {},
   "source": [
    "**Note**: Due to the different values of geolation_lat and geolation_lng, many transactions have been duplicated.Then, We could remove those columns to check the distinct transactions and reduce the size of dataset. Needless to say, Optimizing the dataset size is also an important task that need to consider to optimize the data processing speed. As in the example below, filtering the disctint rows in a large dataset could easily lead to the out-of-memory error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fc2421bc",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o85.count.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 7.0 failed 1 times, most recent failure: Lost task 0.0 in stage 7.0 (TID 53, DESKTOP-RG2MEJH, executor driver): java.lang.OutOfMemoryError: Java heap space\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2059)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2008)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2007)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2007)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:973)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:973)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:973)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2239)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2188)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2177)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:775)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2114)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2135)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2154)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2179)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1004)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1003)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:390)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$count$1(Dataset.scala:2981)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$count$1$adapted(Dataset.scala:2980)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3618)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:767)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3616)\r\n\tat org.apache.spark.sql.Dataset.count(Dataset.scala:2980)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.lang.OutOfMemoryError: Java heap space\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_27376/2849727983.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistinct\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Spark\\spark3\\python\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mcount\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    583\u001b[0m         \u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    584\u001b[0m         \"\"\"\n\u001b[1;32m--> 585\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    586\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    587\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mignore_unicode_prefix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Spark\\spark3\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1304\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Spark\\spark3\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    126\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Spark\\spark3\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m                 raise Py4JJavaError(\n\u001b[0m\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o85.count.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 7.0 failed 1 times, most recent failure: Lost task 0.0 in stage 7.0 (TID 53, DESKTOP-RG2MEJH, executor driver): java.lang.OutOfMemoryError: Java heap space\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2059)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2008)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2007)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2007)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:973)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:973)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:973)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2239)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2188)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2177)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:775)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2114)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2135)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2154)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2179)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1004)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1003)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:390)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$count$1(Dataset.scala:2981)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$count$1$adapted(Dataset.scala:2980)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3618)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:767)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3616)\r\n\tat org.apache.spark.sql.Dataset.count(Dataset.scala:2980)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.lang.OutOfMemoryError: Java heap space\r\n"
     ]
    }
   ],
   "source": [
    "df = data.distinct()\n",
    "df.count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2cea3c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66dfa12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop the uncessary columns\n",
    "dropped_geo_data = data.drop(*[\"geolocation_lat\",\"geolocation_lng\",\"geolocation_city\",\"geolocation_state\",\"customer_zip_code_prefix\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2dfe8086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Number of Records: integer (nullable = true)\n",
      " |-- amount: integer (nullable = true)\n",
      " |-- customer_city: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- customer_state: string (nullable = true)\n",
      " |-- customer_unique_id: string (nullable = true)\n",
      " |-- freight_value: double (nullable = true)\n",
      " |-- index: integer (nullable = true)\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- order_purchase_timestamp: string (nullable = true)\n",
      " |-- order_year_and_month: string (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- product_category_name_english: string (nullable = true)\n",
      " |-- product_id: string (nullable = true)\n",
      " |-- seller_Cluster: string (nullable = true)\n",
      " |-- seller_city: string (nullable = true)\n",
      " |-- seller_id: string (nullable = true)\n",
      " |-- seller_revenue: double (nullable = true)\n",
      " |-- seller_state: string (nullable = true)\n",
      " |-- seller_zip_code_prefix: integer (nullable = true)\n",
      " |-- shipping_limit_date: string (nullable = true)\n",
      " |-- sum_seller_revenue: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#check the Schema after dropping the uncessary columns\n",
    "dropped_geo_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18ed6a20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct count: 32951\n"
     ]
    }
   ],
   "source": [
    "# Remove the duplication by using distinct method\n",
    "distinctDF = dropped_geo_data.distinct()\n",
    "print(\"Distinct count: \"+str(distinctDF.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbba228",
   "metadata": {},
   "source": [
    "**Note**: Voila, after removing the dupplication, there is only distinct 32951 rows, it is much easier to analyse right now. Due to the immutability of Spark dataframe, it is better to save the modified dataframe to the new object and runnning the count to let Spark doing the transformation (lazy processing).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c1df80",
   "metadata": {},
   "source": [
    "#### Question 1: What are the number of order per customer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b868fd19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------+\n",
      "|         customer_id|count(order_id)|\n",
      "+--------------------+---------------+\n",
      "|ff3cc011a4b83b12d...|              1|\n",
      "|5d0244490e084d6d7...|              1|\n",
      "|1b47e01868a3afb43...|              1|\n",
      "|638042a6d4935fd75...|              1|\n",
      "|3dd2f8a8b58ffd06e...|              1|\n",
      "|57458ec5d4b1bcb40...|              1|\n",
      "|07fbc7cbc612a1302...|              1|\n",
      "|a480d7510ff88ed0d...|              1|\n",
      "|25fcb9904c37d3521...|              1|\n",
      "|db9f32899c9b8907a...|              1|\n",
      "|b9c4b8634e1ee526d...|              1|\n",
      "|352438065e364ccbd...|              1|\n",
      "|16d9b76f2e917b32d...|              1|\n",
      "|5f6478ee9ad1339b3...|              1|\n",
      "|0b70ffeae43a2bb3a...|              1|\n",
      "|74a6b307569709700...|              1|\n",
      "|87f4ffa28a4526f3e...|              1|\n",
      "|78d601a14d2991363...|              1|\n",
      "|9dd46d53f1d993cb9...|              1|\n",
      "|cd0fd4a64915d508f...|              1|\n",
      "+--------------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#check the number of order by customer\n",
    "distinctDF.groupby(\"customer_id\").agg(countDistinct(\"order_id\")).orderBy(\"count(order_id)\",ascending = False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff392ee2",
   "metadata": {},
   "source": [
    "**Note**: It seems that each customer only have a single transaction in this dataset. In practical, each customer might have multiples orders. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b5fc61",
   "metadata": {},
   "source": [
    "#### Question 2: What are the best selling product categories?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3543b8f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of product category is: 72\n"
     ]
    }
   ],
   "source": [
    "#check the number of product category\n",
    "print(\"Number of product category is: \"+str(distinctDF.select(\"product_category_name_english\").distinct().count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "226a9f3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_category_name_english</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bed_bath_table</td>\n",
       "      <td>3029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sports_leisure</td>\n",
       "      <td>2867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>furniture_decor</td>\n",
       "      <td>2657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>health_beauty</td>\n",
       "      <td>2444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>housewares</td>\n",
       "      <td>2335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>auto</td>\n",
       "      <td>1900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>computers_accessories</td>\n",
       "      <td>1639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>toys</td>\n",
       "      <td>1411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>watches_gifts</td>\n",
       "      <td>1329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>telephony</td>\n",
       "      <td>1134</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  product_category_name_english  count\n",
       "0                bed_bath_table   3029\n",
       "1                sports_leisure   2867\n",
       "2               furniture_decor   2657\n",
       "3                 health_beauty   2444\n",
       "4                    housewares   2335\n",
       "5                          auto   1900\n",
       "6         computers_accessories   1639\n",
       "7                          toys   1411\n",
       "8                 watches_gifts   1329\n",
       "9                     telephony   1134"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check the best-selling product categories\n",
    "distinctDF.groupby([\"product_category_name_english\"]).count().orderBy(\"count\",ascending = False).limit(10).toPandas()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43145c6",
   "metadata": {},
   "source": [
    "**Note**: It is great to know the most popular categories in the e-commerce platform, however the more important is finding the causes of those selling. It could be that the platform is advertising those sport items vividly and targeting to the right segment. On the contrary, By looking at categories which did not selling well, the CEO could change the current business plan such as allocating more budget to improve content and images, besides thoroughtly collecting the reviews and feedbacks from the users."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477ac11c",
   "metadata": {},
   "source": [
    "#### Question 3: Which period has the most sell?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d6abd7",
   "metadata": {},
   "source": [
    "To answer this question, the first step is converting order_purchase_timestamp columns from string into datetimes data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "62524bd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Number of Records: integer (nullable = true)\n",
      " |-- amount: integer (nullable = true)\n",
      " |-- customer_city: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- customer_state: string (nullable = true)\n",
      " |-- customer_unique_id: string (nullable = true)\n",
      " |-- freight_value: double (nullable = true)\n",
      " |-- index: integer (nullable = true)\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- order_purchase_timestamp: string (nullable = true)\n",
      " |-- order_year_and_month: string (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- product_category_name_english: string (nullable = true)\n",
      " |-- product_id: string (nullable = true)\n",
      " |-- seller_Cluster: string (nullable = true)\n",
      " |-- seller_city: string (nullable = true)\n",
      " |-- seller_id: string (nullable = true)\n",
      " |-- seller_revenue: double (nullable = true)\n",
      " |-- seller_state: string (nullable = true)\n",
      " |-- seller_zip_code_prefix: integer (nullable = true)\n",
      " |-- shipping_limit_date: string (nullable = true)\n",
      " |-- sum_seller_revenue: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#check current data schema\n",
    "distinctDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "56ba8bc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+\n",
      "|order_purchase_timestamp|\n",
      "+------------------------+\n",
      "|    20/08/2017 11:40:...|\n",
      "|    13/01/2018 9:59:4...|\n",
      "|    23/08/2017 2:29:4...|\n",
      "|    23/09/2017 2:56:4...|\n",
      "|    17/07/2017 4:04:5...|\n",
      "|    18/03/2018 6:35:3...|\n",
      "|    31/05/2018 5:47:2...|\n",
      "|    11/08/2018 10:52:...|\n",
      "|    20/03/2018 2:27:2...|\n",
      "|    19/05/2017 3:48:1...|\n",
      "|    18/05/2018 12:32:...|\n",
      "|    25/06/2017 5:11:0...|\n",
      "|    26/01/2018 5:00:1...|\n",
      "|    12/03/2017 5:42:5...|\n",
      "|    6/08/2017 8:04:28 PM|\n",
      "|    8/06/2018 2:32:25 PM|\n",
      "|    9/08/2018 7:17:50 PM|\n",
      "|    9/03/2017 4:18:47 PM|\n",
      "|    30/10/2017 10:40:...|\n",
      "|    6/09/2017 10:19:1...|\n",
      "+------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Review the pattern of order_purchase_timestamp column\n",
    "distinctDF.select(\"order_purchase_timestamp\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ef9682be",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\",\"LEGACY\") #define the timeParserPolicy to deal with dates\n",
    "date_time_pattern = \"dd/MM/yyyy hh:mm:ss aa\" #current datetime pattern\n",
    "\n",
    "#convert the string data to datetime \n",
    "distinctedDF = distinctDF.withColumn(\"order_purchase_timestamp_converted\", unix_timestamp(distinctDF[\"order_purchase_timestamp\"], date_time_pattern).cast(\"timestamp\")).drop(\"order_purchase_timestamp\")\n",
    "#distinctedDF = distinctDF.withColumn(\"order_purchase_timestamp_converted\",F.to_timestamp(distinctDF[\"order_purchase_timestamp\",date_time_pattern])).drop(\"order_purchase_timestamp\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06a1729",
   "metadata": {},
   "source": [
    "**Note**: In a newer version of spark( > 3.0), it will fail to recognize 'dd/MM/yyyy hh:mm:ss aa' pattern in the DateTimeFormatter, thus need to set spark.conf.set(\"spark.sql.legacy.timeParserPolicy\",\"LEGACY\").\n",
    "Read more: https://www.waitingforcode.com/apache-spark-sql/whats-new-apache-spark-3-proleptic-calendar-date-time-management/read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4b465940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Number of Records: integer (nullable = true)\n",
      " |-- amount: integer (nullable = true)\n",
      " |-- customer_city: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- customer_state: string (nullable = true)\n",
      " |-- customer_unique_id: string (nullable = true)\n",
      " |-- freight_value: double (nullable = true)\n",
      " |-- index: integer (nullable = true)\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- order_year_and_month: string (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- product_category_name_english: string (nullable = true)\n",
      " |-- product_id: string (nullable = true)\n",
      " |-- seller_Cluster: string (nullable = true)\n",
      " |-- seller_city: string (nullable = true)\n",
      " |-- seller_id: string (nullable = true)\n",
      " |-- seller_revenue: double (nullable = true)\n",
      " |-- seller_state: string (nullable = true)\n",
      " |-- seller_zip_code_prefix: integer (nullable = true)\n",
      " |-- shipping_limit_date: string (nullable = true)\n",
      " |-- sum_seller_revenue: double (nullable = true)\n",
      " |-- order_purchase_timestamp_converted: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#chech the Schema of the new dataframe: distinctedDF\n",
    "distinctedDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7fbb15b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------+---------------------------------------+\n",
      "|min(order_purchase_timestamp_converted)|max(order_purchase_timestamp_converted)|\n",
      "+---------------------------------------+---------------------------------------+\n",
      "|                    2016-09-04 21:15:19|                    2018-08-29 14:52:00|\n",
      "+---------------------------------------+---------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#check the period of records\n",
    "distinctedDF.select(min(\"order_purchase_timestamp_converted\"),max(\"order_purchase_timestamp_converted\")).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186e7f28",
   "metadata": {},
   "source": [
    "**Note**: The data is recorded from 2016 to 2018, in this example, we will process the data from 2017 onwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4102a925",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Number of Records</th>\n",
       "      <th>amount</th>\n",
       "      <th>customer_city</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>customer_state</th>\n",
       "      <th>customer_unique_id</th>\n",
       "      <th>freight_value</th>\n",
       "      <th>index</th>\n",
       "      <th>order_id</th>\n",
       "      <th>order_year_and_month</th>\n",
       "      <th>...</th>\n",
       "      <th>product_id</th>\n",
       "      <th>seller_Cluster</th>\n",
       "      <th>seller_city</th>\n",
       "      <th>seller_id</th>\n",
       "      <th>seller_revenue</th>\n",
       "      <th>seller_state</th>\n",
       "      <th>seller_zip_code_prefix</th>\n",
       "      <th>shipping_limit_date</th>\n",
       "      <th>sum_seller_revenue</th>\n",
       "      <th>order_purchase_timestamp_converted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Curitiba</td>\n",
       "      <td>d2c63ad286e3ca9dd69218008d61ff81</td>\n",
       "      <td>PR</td>\n",
       "      <td>728e4a7d0db4845270091ded0923c71f</td>\n",
       "      <td>8.72</td>\n",
       "      <td>47037</td>\n",
       "      <td>38bcb524e1c38c2c1b60600a80fc8999</td>\n",
       "      <td>1/01/2017</td>\n",
       "      <td>...</td>\n",
       "      <td>680cc8535be7cc69544238c1d6a83fe8</td>\n",
       "      <td>Cluster 0</td>\n",
       "      <td>Curitiba</td>\n",
       "      <td>48efc9d94a9834137efd9ea76b065a38</td>\n",
       "      <td>2.9</td>\n",
       "      <td>PR</td>\n",
       "      <td>81130</td>\n",
       "      <td>9/01/2017 12:06:36 PM</td>\n",
       "      <td>187.2</td>\n",
       "      <td>2017-01-05 12:06:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Curitiba</td>\n",
       "      <td>42d857e22f61f10476f0cda1aacc018a</td>\n",
       "      <td>PR</td>\n",
       "      <td>728e4a7d0db4845270091ded0923c71f</td>\n",
       "      <td>8.72</td>\n",
       "      <td>8785</td>\n",
       "      <td>6acecf438369055d9243e121045cca74</td>\n",
       "      <td>1/01/2017</td>\n",
       "      <td>...</td>\n",
       "      <td>1514ddb0f4a5afc8d24104e89c714403</td>\n",
       "      <td>Cluster 0</td>\n",
       "      <td>Curitiba</td>\n",
       "      <td>48efc9d94a9834137efd9ea76b065a38</td>\n",
       "      <td>9.9</td>\n",
       "      <td>PR</td>\n",
       "      <td>81130</td>\n",
       "      <td>9/01/2017 12:11:23 PM</td>\n",
       "      <td>187.2</td>\n",
       "      <td>2017-01-05 12:11:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Curitiba</td>\n",
       "      <td>efdf4a7c78d7c364046efb69035d1d4f</td>\n",
       "      <td>PR</td>\n",
       "      <td>ef89f6e31311594d74becf9e18c73693</td>\n",
       "      <td>8.72</td>\n",
       "      <td>82441</td>\n",
       "      <td>40599d3d28b75746952ded75566637b9</td>\n",
       "      <td>1/01/2017</td>\n",
       "      <td>...</td>\n",
       "      <td>baddee1245c9736a49167f66e0912b0d</td>\n",
       "      <td>Cluster 0</td>\n",
       "      <td>Curitiba</td>\n",
       "      <td>48efc9d94a9834137efd9ea76b065a38</td>\n",
       "      <td>11.9</td>\n",
       "      <td>PR</td>\n",
       "      <td>81130</td>\n",
       "      <td>9/01/2017 1:01:48 PM</td>\n",
       "      <td>187.2</td>\n",
       "      <td>2017-01-05 13:01:48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Curitiba</td>\n",
       "      <td>3f402674c608ea67085eb5e0ec4d96ef</td>\n",
       "      <td>PR</td>\n",
       "      <td>83e7958a94bd7f74a9414d8782f87628</td>\n",
       "      <td>8.72</td>\n",
       "      <td>1232</td>\n",
       "      <td>0bda8164c1a12b6a388ebec8559ee287</td>\n",
       "      <td>1/01/2017</td>\n",
       "      <td>...</td>\n",
       "      <td>036bcd084feae22f3a997f5080ec30df</td>\n",
       "      <td>Cluster 0</td>\n",
       "      <td>Curitiba</td>\n",
       "      <td>48efc9d94a9834137efd9ea76b065a38</td>\n",
       "      <td>6.9</td>\n",
       "      <td>PR</td>\n",
       "      <td>81130</td>\n",
       "      <td>9/01/2017 1:36:07 PM</td>\n",
       "      <td>187.2</td>\n",
       "      <td>2017-01-05 13:36:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Curitiba</td>\n",
       "      <td>fc30386d1f0ca6d7f48845275ac0cd23</td>\n",
       "      <td>PR</td>\n",
       "      <td>6222ef69df52b4ed71be551725551cc0</td>\n",
       "      <td>8.72</td>\n",
       "      <td>96777</td>\n",
       "      <td>db7576b1fe440f4c0a808855aacf0948</td>\n",
       "      <td>1/01/2017</td>\n",
       "      <td>...</td>\n",
       "      <td>db9c09ef2fa2590a7ca12480603396aa</td>\n",
       "      <td>Cluster 0</td>\n",
       "      <td>Curitiba</td>\n",
       "      <td>48efc9d94a9834137efd9ea76b065a38</td>\n",
       "      <td>9.9</td>\n",
       "      <td>PR</td>\n",
       "      <td>81130</td>\n",
       "      <td>9/01/2017 1:37:27 PM</td>\n",
       "      <td>187.2</td>\n",
       "      <td>2017-01-05 13:37:27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Number of Records  amount customer_city                       customer_id  \\\n",
       "0                  1       1      Curitiba  d2c63ad286e3ca9dd69218008d61ff81   \n",
       "1                  1       1      Curitiba  42d857e22f61f10476f0cda1aacc018a   \n",
       "2                  1       1      Curitiba  efdf4a7c78d7c364046efb69035d1d4f   \n",
       "3                  1       1      Curitiba  3f402674c608ea67085eb5e0ec4d96ef   \n",
       "4                  1       1      Curitiba  fc30386d1f0ca6d7f48845275ac0cd23   \n",
       "\n",
       "  customer_state                customer_unique_id  freight_value  index  \\\n",
       "0             PR  728e4a7d0db4845270091ded0923c71f           8.72  47037   \n",
       "1             PR  728e4a7d0db4845270091ded0923c71f           8.72   8785   \n",
       "2             PR  ef89f6e31311594d74becf9e18c73693           8.72  82441   \n",
       "3             PR  83e7958a94bd7f74a9414d8782f87628           8.72   1232   \n",
       "4             PR  6222ef69df52b4ed71be551725551cc0           8.72  96777   \n",
       "\n",
       "                           order_id order_year_and_month  ...  \\\n",
       "0  38bcb524e1c38c2c1b60600a80fc8999            1/01/2017  ...   \n",
       "1  6acecf438369055d9243e121045cca74            1/01/2017  ...   \n",
       "2  40599d3d28b75746952ded75566637b9            1/01/2017  ...   \n",
       "3  0bda8164c1a12b6a388ebec8559ee287            1/01/2017  ...   \n",
       "4  db7576b1fe440f4c0a808855aacf0948            1/01/2017  ...   \n",
       "\n",
       "                         product_id seller_Cluster seller_city  \\\n",
       "0  680cc8535be7cc69544238c1d6a83fe8      Cluster 0    Curitiba   \n",
       "1  1514ddb0f4a5afc8d24104e89c714403      Cluster 0    Curitiba   \n",
       "2  baddee1245c9736a49167f66e0912b0d      Cluster 0    Curitiba   \n",
       "3  036bcd084feae22f3a997f5080ec30df      Cluster 0    Curitiba   \n",
       "4  db9c09ef2fa2590a7ca12480603396aa      Cluster 0    Curitiba   \n",
       "\n",
       "                          seller_id seller_revenue seller_state  \\\n",
       "0  48efc9d94a9834137efd9ea76b065a38            2.9           PR   \n",
       "1  48efc9d94a9834137efd9ea76b065a38            9.9           PR   \n",
       "2  48efc9d94a9834137efd9ea76b065a38           11.9           PR   \n",
       "3  48efc9d94a9834137efd9ea76b065a38            6.9           PR   \n",
       "4  48efc9d94a9834137efd9ea76b065a38            9.9           PR   \n",
       "\n",
       "   seller_zip_code_prefix    shipping_limit_date  sum_seller_revenue  \\\n",
       "0                   81130  9/01/2017 12:06:36 PM               187.2   \n",
       "1                   81130  9/01/2017 12:11:23 PM               187.2   \n",
       "2                   81130   9/01/2017 1:01:48 PM               187.2   \n",
       "3                   81130   9/01/2017 1:36:07 PM               187.2   \n",
       "4                   81130   9/01/2017 1:37:27 PM               187.2   \n",
       "\n",
       "  order_purchase_timestamp_converted  \n",
       "0                2017-01-05 12:06:36  \n",
       "1                2017-01-05 12:11:23  \n",
       "2                2017-01-05 13:01:48  \n",
       "3                2017-01-05 13:36:07  \n",
       "4                2017-01-05 13:37:27  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Filter the data from 2017 to 2018\n",
    "distinctedDF.filter(distinctedDF[\"order_purchase_timestamp_converted\"] > lit(\"2017-01-01 00:00:00\")) \\\n",
    "    .orderBy(\"order_purchase_timestamp_converted\",ascending = True).limit(5).toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "6d332ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count the transaction by month\n",
    "transaction_count_df = distinctedDF.filter(distinctedDF[\"order_purchase_timestamp_converted\"] > lit(\"2017-01-01 00:00:00\")) \\\n",
    "    .groupBy(month(\"order_purchase_timestamp_converted\").alias(\"month\"),year(\"order_purchase_timestamp_converted\").alias(\"year\")) \\\n",
    "    .agg(count(\"order_purchase_timestamp_converted\").alias(\"count_transaction\")) \\\n",
    "    .orderBy(col(\"month\").asc(),col(\"year\").asc()) \n",
    "    #.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "6ba9fd75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- month: integer (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- count_transaction: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#check the transaction_count_df schema\n",
    "transaction_count_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "8cba80fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+-----------------+\n",
      "|month|year|count_transaction|\n",
      "+-----+----+-----------------+\n",
      "|    8|2018|             2771|\n",
      "|    7|2018|             2529|\n",
      "|   11|2017|             2455|\n",
      "|    6|2018|             2283|\n",
      "|    4|2018|             2233|\n",
      "|    5|2018|             2182|\n",
      "|    1|2018|             2061|\n",
      "|    3|2018|             2037|\n",
      "|    2|2018|             1864|\n",
      "|   12|2017|             1674|\n",
      "|   10|2017|             1505|\n",
      "|    8|2017|             1445|\n",
      "|    9|2017|             1379|\n",
      "|    5|2017|             1266|\n",
      "|    7|2017|             1220|\n",
      "|    3|2017|              990|\n",
      "|    6|2017|              944|\n",
      "|    4|2017|              811|\n",
      "|    2|2017|              737|\n",
      "|    1|2017|              366|\n",
      "+-----+----+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#sort the dataframe descendingly to check the top transaction\n",
    "transaction_count_df.sort(col(\"count_transaction\").desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37834dd9",
   "metadata": {},
   "source": [
    "**Note**: we could see that, the platform is growing by the time. Standing out the most is the number of transaction in November 2017 which is significantly higher than the other months in 2017. It might be that, the company run a big promotion during that time to attract more users. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd0951e",
   "metadata": {},
   "source": [
    "In Spark, we could use SQL query to do the analysis also and in this example, we could try to run the query on the transaction of 2018."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "9ee02103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+-----------------+\n",
      "|month|year|count_transaction|\n",
      "+-----+----+-----------------+\n",
      "|    2|2018|             1864|\n",
      "|    3|2018|             2037|\n",
      "|    1|2018|             2061|\n",
      "|    5|2018|             2182|\n",
      "|    4|2018|             2233|\n",
      "|    6|2018|             2283|\n",
      "|    7|2018|             2529|\n",
      "|    8|2018|             2771|\n",
      "+-----+----+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#create a view to run the query on\n",
    "transaction_count_df.createOrReplaceTempView(\"transaction_count_table\")\n",
    "\n",
    "#set the query and run on Spark\n",
    "query = \"SELECT month,year, count_transaction FROM transaction_count_table WHERE year = 2018 ORDER BY count_transaction \"\n",
    "count_transaction_2018 = spark.sql(query)\n",
    "count_transaction_2018.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ce4c71",
   "metadata": {},
   "source": [
    "To visualize the data, we will convert the dataframe to pandas and utilize matplotlib library "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "a2ff105d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#Convert dataframe to pandas to do the visualization\n",
    "count_transaction_2018_pandas = count_transaction_2018.toPandas()\n",
    "\n",
    "#Draw the bar plot \n",
    "count_transaction_2018_pandas.plot(kind='barh', x='month', y='count_transaction', colormap='winter_r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a76a8f1",
   "metadata": {},
   "source": [
    "**Note**: As the mentined above,  the pyspark dataframe is immutable, the example below could illustrate the point clearer by comparing the id of the count_transaction_2018 dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "a339e609",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "666"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check the rdd id of count_transaction_2018\n",
    "count_transaction_2018.rdd.id()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "028b73b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+-----------------+-------------------+\n",
      "|month|year|count_transaction|measure_transaction|\n",
      "+-----+----+-----------------+-------------------+\n",
      "|    2|2018|             1864|         acceptable|\n",
      "|    3|2018|             2037|         acceptable|\n",
      "|    1|2018|             2061|         acceptable|\n",
      "|    5|2018|             2182|         acceptable|\n",
      "|    4|2018|             2233|         acceptable|\n",
      "|    6|2018|             2283|         acceptable|\n",
      "|    7|2018|             2529|         acceptable|\n",
      "|    8|2018|             2771|         acceptable|\n",
      "+-----+----+-----------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "#create a function to measure the amount of transaction\n",
    "def measure_transaction(number_of_transaction):\n",
    "    \"\"\" we rank the performance of the month by the number of transaction\n",
    "        > 2000: acceptable \n",
    "        < 2000: low \"\"\"\n",
    "    if number_of_transaction < 2000:\n",
    "        return(\"low\")\n",
    "    return(\"acceptable\")\n",
    "\n",
    "#convert python function into udf function\n",
    "udfmeasure_transaction = udf(measure_transaction,StringType())\n",
    "count_transaction_2018 = count_transaction_2018.withColumn(\"measure_transaction\",udfmeasure_transaction(count_transaction_2018.count_transaction))\n",
    "count_transaction_2018.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569d4909",
   "metadata": {},
   "source": [
    "**Note**: we could also use annotation to convert the python function to udf function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "8f3e4f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+-----------------+-------------------+\n",
      "|month|year|count_transaction|measure_transaction|\n",
      "+-----+----+-----------------+-------------------+\n",
      "|    2|2018|             1864|                low|\n",
      "|    3|2018|             2037|         acceptable|\n",
      "|    1|2018|             2061|         acceptable|\n",
      "|    5|2018|             2182|         acceptable|\n",
      "|    4|2018|             2233|         acceptable|\n",
      "|    6|2018|             2283|         acceptable|\n",
      "|    7|2018|             2529|         acceptable|\n",
      "|    8|2018|             2771|         acceptable|\n",
      "+-----+----+-----------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "@udf(returnType=StringType())\n",
    "def measure_transaction(number_of_transaction):\n",
    "    if number_of_transaction < 2000:\n",
    "        return(\"low\")\n",
    "    return(\"acceptable\")\n",
    "\n",
    "#udfmeasure_transaction = udf(measure_transtraction,StringType())\n",
    "count_transaction_2018 = count_transaction_2018.withColumn(\"measure_transaction\",measure_transaction(count_transaction_2018.count_transaction))\n",
    "count_transaction_2018.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4c6ca2",
   "metadata": {},
   "source": [
    " In Spark, when we do either adding a column or dropping one actually, we are creating a new Dataset/Dataframe, we could see the difference by checking the rdd ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "5bd003b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "746"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check the rdd ID of the count_transaction_2018 dataframe\n",
    "count_transaction_2018.rdd.id()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf68a19",
   "metadata": {},
   "source": [
    "### Conclusion:\n",
    "    By using PySpark, we could handle the bid data more efficiently. However, There are some differences to pandas dataframe, that need to pay attention to such as the immutable dataframe, the lazy processing, SQL query.\n",
    "\n",
    "    In the next step, we could try to run Spark on cloud, such as Google Data Proc and using Spark in Google BigQuery.\n",
    "\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
